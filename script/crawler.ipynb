{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4w5tgI2-D-7h"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup, NavigableString\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Batchurls():\n",
        "\n",
        "  def __init__(self,*pages):\n",
        "    templete = 'https://answers.justia.com/questions/answered/california?page='\n",
        "    self.docs = [templete + str(i) for i in range(pages[0],pages[1]+1)]\n",
        "\n",
        "    f = lambda x: BeautifulSoup(self.get_html(x))\n",
        "    self.docs = list(map(f,self.docs))\n",
        "\n",
        "  def get_html(self, url):\n",
        "    x = requests.get(url)\n",
        "    return x.text\n",
        "\n",
        "  def update_doc(self):\n",
        "    f = lambda x: BeautifulSoup(self.get_html(x))\n",
        "    self.docs = list(map(f,self.postlinks))\n",
        "\n",
        "  def get_qlink(self):\n",
        "    self.postlinks = []\n",
        "    templete = 'https://answers.justia.com'\n",
        "\n",
        "    for doc in self.docs:\n",
        "      for s in doc.find_all('strong'):\n",
        "          if re.match('(Q:)',str(s.next_element)):\n",
        "            link = templete + str(s.find('a')['href'])\n",
        "            self.postlinks.append(link)"
      ],
      "metadata": {
        "id": "UBXK2RpxJWeC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This part will take some time depedning on the amount of pages you want to scrape\n",
        "urls = Batchurls(2,12) #Enter page number from 2 to some number more than 2(I didn't check the end of this website, but I was able to find over 500 page)\n",
        "urls.get_qlink()\n",
        "urls.update_doc()"
      ],
      "metadata": {
        "id": "4LRu6cOhJnRP"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = {'question':[],'answer':[]}\n",
        "for doc in urls.docs:\n",
        "  string_question = doc.find('title').text\n",
        "  string = doc.find_all('p')[0].text\n",
        "  if re.search('(A\\:)',string):\n",
        "    string_answer = string\n",
        "  else:\n",
        "    string_question += string\n",
        "    string_answer = doc.find_all('p')[1].text\n",
        "\n",
        "  dic['question'].append(string_question)\n",
        "  dic['answer'].append(string_answer)\n"
      ],
      "metadata": {
        "id": "Y5i4N-2DgI6i"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dic)"
      ],
      "metadata": {
        "id": "Q-lId4S38Xfy"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_json('data.json',orient='records')"
      ],
      "metadata": {
        "id": "HcN7VoKe8bLu"
      },
      "execution_count": 97,
      "outputs": []
    }
  ]
}